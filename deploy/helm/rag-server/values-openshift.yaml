# co-authored with Claude-code
# OpenShift-specific values for NVIDIA RAG Blueprint
# This file overrides values.yaml for OpenShift deployment while preserving original resource names

# Platform identification
platform:
  type: "openshift"

# Keep original app name and namespace
appName: rag-server
namespace: "nv-nvidia-blueprint-rag"

# OpenShift-compatible security contexts (no hardcoded UIDs)
securityContext:
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  seccompProfile:
    type: RuntimeDefault

podSecurityContext:
  runAsNonRoot: true
  seccompProfile:
    type: RuntimeDefault

# Reduced resource requirements for learning/dev environments
resources:
  limits:
    memory: "8Gi"    # Reduced from 64Gi
    cpu: "2"
  requests:
    memory: "2Gi"    # Reduced from 8Gi  
    cpu: "500m"

envVars:
  EXAMPLE_PATH: "./nvidia_rag/rag_server"
  PROMPT_CONFIG_FILE: "/prompt.yaml"

  ##===MINIO specific configurations===
  MINIO_ENDPOINT: "rag-minio:9000"
  MINIO_ACCESSKEY: "minioadmin"
  MINIO_SECRETKEY: "minioadmin"

  ##===Vector DB specific configurations===
  APP_VECTORSTORE_URL: "http://milvus:19530"
  APP_VECTORSTORE_NAME: "milvus"
  APP_VECTORSTORE_SEARCHTYPE: "dense"
  APP_VECTORSTORE_CONSISTENCYLEVEL: "Strong"
  COLLECTION_NAME: "multimodal_data"
  APP_RETRIEVER_SCORETHRESHOLD: "0.25"
  VECTOR_DB_TOPK: "100"
  APP_RETRIEVER_TOPK: "10"

  # vector database settings
  APP_VECTORSTORE_ENABLEGPUINDEX: "True"
  APP_VECTORSTORE_ENABLEGPUSEARCH: "True"

  ##===LLM Model specific configurations===
  APP_LLM_MODELNAME: "nvidia/llama-3.3-nemotron-super-49b-v1"
  APP_LLM_SERVERURL: "nim-llm:8000"

  ##===Query Rewriter Model specific configurations===
  APP_QUERYREWRITER_MODELNAME: "meta/llama-3.1-8b-instruct"
  APP_QUERYREWRITER_SERVERURL: "nim-llm-llama-8b:8000"

  ##===Embedding Model specific configurations===
  APP_EMBEDDINGS_SERVERURL: "nemoretriever-embedding-ms:8000"
  APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"

  ##===Reranking Model specific configurations===
  APP_RANKING_SERVERURL: "nemoretriever-reranking-ms:8000"
  APP_RANKING_MODELNAME: "nvidia/llama-3.2-nv-rerankqa-1b-v2"
  ENABLE_RERANKER: "True"  # Disabled for CPU-only

  # === Text Splitter ===
  APP_TEXTSPLITTER_CHUNKSIZE: "2000"
  APP_TEXTSPLITTER_CHUNKOVERLAP: "200"

  # === General ===
  ENABLE_CITATIONS: "True"
  ENABLE_GUARDRAILS: "False"  # Disabled for CPU-only
  LOGLEVEL: "INFO"
  ENABLE_MULTITURN: "True"
  ENABLE_QUERYREWRITER: "False"  # Disabled for CPU-only
  ENABLE_REFLECTION: "False"     # Disabled for CPU-only
  ENABLE_VLM_INFERENCE: "false"  # Disabled for CPU-only

  # === Multi-Modal Settings ===
  ENABLE_MULTIMODAL: "False"  # Disabled for CPU-only

  # === Tracing Configuration ===
  APP_TRACING_ENABLED: "True"
  APP_TRACING_OTLPHTTPENDPOINT: "http://opentelemetry-collector:4318/v1/traces"
  APP_TRACING_OTLPGRPCENDPOINT: "grpc://opentelemetry-collector:4317"

  # === NeMo Guardrails ===
  NEMO_GUARDRAILS_URL: "nemo-guardrails-microservice:7331"

# Ingestor Server OpenShift Configuration
ingestor-server:
  enabled: true
  replicaCount: 1

  # Reduced resources
  resources:
    limits:
      memory: "10Gi"    
    requests:
      memory: "5Gi"    

  envVars:
    # === Vector Store Configurations ===
    APP_VECTORSTORE_URL: "http://milvus:19530"
    APP_VECTORSTORE_NAME: "milvus"
    APP_VECTORSTORE_SEARCHTYPE: "dense"
    APP_VECTORSTORE_ENABLEGPUINDEX: "False"
    APP_VECTORSTORE_ENABLEGPUSEARCH: "False"
    COLLECTION_NAME: "multimodal_data"

    # === MinIO Configurations ===
    MINIO_ENDPOINT: "rag-minio:9000"
    MINIO_ACCESSKEY: "minioadmin"
    MINIO_SECRETKEY: "minioadmin"

    # === Embeddings Configurations ===
    APP_EMBEDDINGS_SERVERURL: "nemoretriever-embedding-ms:8000"
    APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    APP_EMBEDDINGS_DIMENSIONS: "2048"

    # === NV-Ingest Configurations ===
    APP_NVINGEST_MESSAGECLIENTHOSTNAME: "rag-nv-ingest"
    APP_NVINGEST_MESSAGECLIENTPORT: "7670"

    # === NV-Ingest extraction configurations (CPU-only) ===
    APP_NVINGEST_PDFEXTRACTMETHOD: "None"  # Disabled for CPU-only
    APP_NVINGEST_EXTRACTTEXT: "True"
    APP_NVINGEST_EXTRACTINFOGRAPHICS: "False"  # Disabled for CPU-only
    APP_NVINGEST_EXTRACTTABLES: "False"        # Disabled for CPU-only
    APP_NVINGEST_EXTRACTCHARTS: "False"        # Disabled for CPU-only
    APP_NVINGEST_EXTRACTIMAGES: "False"        # Disabled for CPU-only
    APP_NVINGEST_TEXTDEPTH: "page"

    # === NV-Ingest caption configurations ===
    APP_NVINGEST_CAPTIONMODELNAME: "nvidia/llama-3.1-nemotron-nano-vl-8b-v1"
    APP_NVINGEST_CAPTIONENDPOINTURL: ""

    # === General ===
    SUMMARY_LLM: "nvidia/llama-3.3-nemotron-super-49b-v1"
    SUMMARY_LLM_SERVERURL: "nim-llm:8000"
    SUMMARY_LLM_MAX_CHUNK_LENGTH: "50000"
    SUMMARY_CHUNK_OVERLAP: "200"
    ENABLE_CITATIONS: "True"
    LOGLEVEL: "INFO"

    # === NV-Ingest splitting configurations ===
    APP_NVINGEST_CHUNKSIZE: "512"
    APP_NVINGEST_CHUNKOVERLAP: "150"
    APP_NVINGEST_ENABLEPDFSPLITTER: "True"

    # === Redis configurations ===
    REDIS_HOST: "rag-redis-master"
    REDIS_PORT: "6379"
    REDIS_DB: "0"

    # === Bulk upload to MinIO ===
    ENABLE_MINIO_BULK_UPLOAD: "True"
    TEMP_DIR: "/tmp-data"

    # === NV-Ingest Batch Mode Configurations ===
    NV_INGEST_FILES_PER_BATCH: "16"
    NV_INGEST_CONCURRENT_BATCHES: "4"

  # NV-Ingest CPU-only configuration
  nv-ingest:
    podSecurityContext:
      fsGroup: 1000
    envVars:
      CONDA_PKGS_DIRS: "/scratch/.cache/conda-pkgs"
      MAMBA_PKGS_DIRS: "/scratch/.cache/conda-pkgs"
      CONDA_BLD_PATH: "/scratch/.cache/conda-bld"
      CONDA_ENVS_PATH: "/scratch/.conda/envs"
      NV_INGEST_MAX_UTIL: "16"
      MAX_INGEST_PROCESS_WORKERS: "8"
    resources:
      limits:
        memory: "200Gi"
        cpu: "16"
      requests:
        memory: "24Gi"
        cpu: "8"

    paddleocr-nim:
      serviceAccount:
        create: true  
      deployed: true  
      replicaCount: 1
      image:
        repository: nvcr.io/nim/baidu/paddleocr
        tag: "1.3.0"
      imagePullSecrets:
      - name: ngc-secret
      resources:
        limits:
          nvidia.com/gpu: 1
        requests:
          nvidia.com/gpu: 1

    nemoretriever-graphic-elements-v1:
      serviceAccount:
        create: true 
      deployed: true 
      replicaCount: 1
      image:
        repository: nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1
        tag: "1.3.0"
      resources:
        limits:
          nvidia.com/gpu: 1
        requests:
          nvidia.com/gpu: 1

    nemoretriever-page-elements-v2:
      serviceAccount:
        create: true
      deployed: true
      replicaCount: 1
      image:
        repository: nvcr.io/nim/nvidia/nemoretriever-page-elements-v2
        tag: "1.3.0"
      resources:
        limits:
          nvidia.com/gpu: 1
        requests:
          nvidia.com/gpu: 1

    nemoretriever-table-structure-v1:
      serviceAccount:
        create: true
      deployed: true
      replicaCount: 1
      image:
        repository: nvcr.io/nim/nvidia/nemoretriever-table-structure-v1
        tag: "1.3.0"
      resources:
        limits:
          nvidia.com/gpu: 1
        requests:
          nvidia.com/gpu: 1

    # Enable CPU-only Milvus (authentic nv-ingest subchart configuration)
    milvus:
      enabled: true
      fullnameOverride: "milvus"
      image:
        all:
          repository: milvusdb/milvus
          tag: "v2.5.3-gpu"  # CPU version (not v2.5.3-gpu)
          pullPolicy: IfNotPresent
      standalone:
        resources:
          limits:
            nvidia.com/gpu: 1  
            memory: "2Gi"
            cpu: "1"
          requests:
            nvidia.com/gpu: 1  
            memory: "1Gi"
            cpu: "500m"
        extraEnv:
          - name: ETCD_TIMEOUT
            value: "30s"
          - name: STARTUP_TIMEOUT
            value: "60s"
          - name: LOG_LEVEL
            value: "error"
      # Enable embedded etcd for standalone mode
      etcd:
        enabled: true
        replicaCount: 1
        serviceAccount:
          create: true
        resources:
          limits:
            memory: "512Mi"
            cpu: "200m"
          requests:
            memory: "256Mi"
            cpu: "100m"

nim-llm:
  enabled: false  # <-- turn back on
  serviceAccount:
    create: false # <-- turn back on
  resources:
    limits:
      nvidia.com/gpu: 2
      memory: "300Gi"
      cpu: "32"
    requests:
      nvidia.com/gpu: 2
      memory: "300Gi"
      cpu: "16"
  startupProbe:
    enabled: true
    initialDelaySeconds: 120
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 120
    successThreshold: 1
    httpGet:
      path: /v1/health/ready
      port: http-openai
  env:
    - name: QUANTIZATION
      value: "fp16"
  image:
      repository: nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2
      pullPolicy: IfNotPresent
      tag: "latest"
  model:
    ngcAPIKey: ""
    name: "nvidia/nvidia-nemotron-nano-9b-v2"

nim-vlm:
  enabled: false
  serviceAccount:
    create: false

nvidia-nim-llama-32-nv-embedqa-1b-v2:
  enabled: true  
  serviceAccount:
    create: true
  
text-reranking-nim:
  enabled: true  
  serviceAccount:
    create: true

zipkin:
  serviceAccount:
    create: true
  enabled: false
  resources:
    limits:
      memory: "512Mi"
      cpu: "200m"
    requests:
      memory: "256Mi"
      cpu: "100m"

opentelemetry-collector:
  enabled: false
  mode: deployment
  resources:
    limits:
      memory: "256Mi"
      cpu: "100m"
    requests:
      memory: "128Mi"
      cpu: "50m"
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: '${env:MY_POD_IP}:4317'
          http:
            cors:
              allowed_origins:
                - "http://localhost:8090"
                - "http://localhost:3000"
            endpoint: '${env:MY_POD_IP}:4318'
    exporters:
      debug:
        verbosity: detailed
      zipkin:
        endpoint: "http://zipkin:9411/api/v2/spans"
    service:
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [debug, zipkin]
  ports:
    metrics:
      enabled: true
      containerPort: 8889
      servicePort: 8889
      protocol: TCP

# Disable Prometheus stack for now (can be enabled later)
kube-prometheus-stack:
  enabled: false

# Frontend configuration (preserve original naming)
frontend:
  enabled: true
  
  # OpenShift-compatible security contexts
  securityContext:
    runAsNonRoot: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    seccompProfile:
      type: RuntimeDefault

  podSecurityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault

  resources:
    limits:
      memory: "512Mi"
      cpu: "200m"
    requests:
      memory: "256Mi"
      cpu: "100m"
  

  # OpenShift Route configuration (will be added in templates)
  route:
    enabled: true
    host: ""  # Let OpenShift auto-generate
    tls:
      termination: "edge"

  # Disable Kubernetes Ingress when using OpenShift Routes
  ingress:
    enabled: false

  service:
    type: ClusterIP
    port: 3000
    targetPort: 3000

# OpenShift-specific configurations
openshift:
  # Security Context Constraints
  securityContextConstraints:
    enabled: true
    sccName: "anyuid"  # May need to be applied manually

  # Routes configuration
  routes:
    enabled: true
    
  # Disable Kubernetes-specific features
  ingress:
    enabled: false